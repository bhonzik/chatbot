import config # This is config.py, which contains important
              # program information, such as fixed file paths
import nltk   # This is for isolating words and punctuation
import os     # This is for performing file operations
import json   # This is for writing lists directly to files
import ast    # This is for reading lists back from files in a
              # safe manner; the eval() function is not secure
import shutil # This is for deleting non-empty folders
import random # This is for making random selections

def tokenize_files(input_folder, output_folder):

    # This function "tokenizes" or splits up a raw text string into
    # an array of words and punctuation for later processing.
    
    # This function takes a folder path as input ("input_folder"),
    # and it processes each .txt file contained within the folder
    # path, creating a new tokenized .txt file of the same base
    # name within the folder path specified by "output_folder"

    # NOTE: "input_folder" should contain only .txt files and/or
    # subdirectories

    # Delete and re-create "token_folder"
    if os.path.exists(config.token_folder):
        shutil.rmtree(config.token_folder)
    os.makedirs(config.token_folder, exist_ok=True)

    for dir_path, _, file_names in os.walk(input_folder):
        for file_name in file_names:
            file_path = os.path.join(dir_path, file_name)

            # The code above recursively obtains the full file
            # path for each file within "folder" as well as
            # any subdirectory within the "folder" path
            
            # The code below is the process which each .txt file
            # is subjected to

            # Read the file and store the contents in "text"
            with open(file_path, "r") as f:
                text = f.read()

            # Split "text" into a sequence of tokens
            tokenized_text = nltk.word_tokenize(text)

            # Get the base name of the current .txt file
            base_name = os.path.basename(file_path)

            # Create the file path of the new tokenized file
            new_path = os.path.join(output_folder, base_name)

            # Create and write to the new file
            with open(new_path, "w") as f:
                json.dump(tokenized_text, f)

def generate_dictionary(input_folder, output_file_path):

    # This function generates a symbolic dictionary from the
    # the tokenized dataset, associating each token with its
    # own number

    # This function takes a folder path as input ("input_folder"),
    # which should be the same folder specified as "output_folder"
    # for the tokenize_files() function, and a file path
    # ("output_file_path") for the generated dictionary

    # Initialize dictionary as a Python dictionary
    dictionary = {"the": 0}
    next_index = 1

    # Non-recursively obtain the file path of each file in
    # "input_folder"
    # NOTE: This code does not check whether an item is a file
    # or a folder, as the folder generated by tokenize_files()
    # should not include any folders; this code assumes that
    # every item within "input_folder" is a .txt file
    for file_name in os.listdir(input_folder):
        file_path = os.path.join(input_folder, file_name)

        # Read the file and store the contents in "text"
        with open(file_path, "r") as f:
            text = f.read()
            
        # Convert the string "text" to a real Python list
        tokenized_text = ast.literal_eval(text)

        for token in tokenized_text:
            if dictionary.get(token) == None:
                dictionary[str(token)] = next_index
                next_index += 1
    
    # Create and write to dictionary file; overwrite if it
    # already exists
    with open(output_file_path, "w") as f:
        json.dump(dictionary, f)

def dictionarize_files(input_folder, output_folder):

    # This function uses the previously generated dictionary in
    # conjunction with the tokenized data to generate
    # dictionarized data, in which each token is now represented
    # by a unique number; for example, each "hello" token could
    # be replaced with the value 26, or each "goodbye" token could
    # be replaced with the value 89

    # This function takes a folder path as input ("input_folder"),
    # which should be the same folder specified as "output_folder"
    # for the tokenize_files() function, and a folder path
    # "output_folder" for the dictionarized data

    # Delete and re-create "dict_sequence_folder"
    if os.path.exists(config.dictionarized_folder):
        shutil.rmtree(config.dictionarized_folder)
    os.makedirs(config.dictionarized_folder, exist_ok=True)

    # Copy already-existing dictionary as "text"
    with open(config.dictionary_file, "r") as f:
        text = f.read()

    # Initialize dictionary as a Python dictionary
    dictionary = ast.literal_eval(text)

    # Non-recursively obtain the file path of each file in
    # "input_folder"
    # NOTE: This code does not check whether an item is a file
    # or a folder, as the folder generated by tokenize_files()
    # should not include any folders; this code assumes that
    # every item within "input_folder" is a .txt file
    for file_name in os.listdir(input_folder):
        file_path = os.path.join(input_folder, file_name)

        # Initialize dictionarized text as a list
        dictionarized_text = []

        # Read the file and store the contents in "text"
        with open(file_path, "r") as f:
            text = f.read()

        # Convert the string "text" to a real Python list
        tokenized_text = ast.literal_eval(text)

        # Translate "tokenized_text" to "dictionarized_text"
        for token in tokenized_text:
            dictionarized_text.append(dictionary[str(token)])

        # Get the base name of the current .txt file
        base_name = os.path.basename(file_path)

        # Create the file path of the new
        # dictionarized file
        new_path = os.path.join(output_folder, base_name)

        # Create and write to the new file
        with open(new_path, "w") as f:
            json.dump(dictionarized_text, f)
                
def tensorize_files(input_folder, output_folder, sample_rate,
                    sequence_length):

    # This function generates directly trainable tensor data
    # from the previously generated dictionarized text

    # This function takes a folder path as input ("input_folder"),
    # which should be the same folder specified as "output_folder"
    # for the dictionarize_files() function, a folder path
    # "output_folder" for the dictionarized data, a context
    # window size "sequence_length," and a trainable
    # input/output tensor pair generation per token rate
    # "sample_rate"

    # Delete and re-create "tensors" folder
    if os.path.exists(config.tensor_folder):
        shutil.rmtree(config.tensor_folder)
    os.makedirs(config.tensor_folder, exist_ok=True)

    # Non-recursively obtain the file path of each file in
    # "input_folder"
    # NOTE: This code does not check whether an item is a file
    # or a folder, as the folder generated by dictionarize_files()
    # should not include any folders; this code assumes that
    # every item within "input_folder" is a .txt file
    for file_name in os.listdir(input_folder):
        file_path = os.path.join(input_folder, file_name)

        # Read the file and store the contents in "text"
        with open(file_path, "r") as f:
            text = f.read()

        # Convert the string "text" to a real Python list
        dictionarized_text = ast.literal_eval(text)

        # Calculate sample count
        sample_count = round(len(dictionarized_text) * sample_rate)

        # Initialize tensor list as "tensors"
        tensors = []

        # Define buffer size in order to avoid index out of
        # range errors
        buffer = sequence_length + 1

        for sample in range(sample_count):

            X_Y_pair = []
            X_train = []
            Y_train = []

            sample_index = random.randint(0, len(dictionarized_text) - buffer)

            for token in range(sequence_length):
                
                X_train.append(dictionarized_text[sample_index])
                sample_index += 1
            
            Y_train.append(dictionarized_text[sample_index])

            X_Y_pair.append(X_train)
            X_Y_pair.append(Y_train)

            tensors.append(X_Y_pair)

        # Get the base name of the current .txt file
        base_name = os.path.basename(file_path)

        # Create the file path of the new
        # dictionarized file
        new_path = os.path.join(output_folder, base_name)

        # Create and write to the new file
        with open(new_path, "w") as f:
            json.dump(tensors, f)