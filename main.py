import config # This is config.py, which contains important
              # program information, such as fixed file paths
import nltk   # This is for isolating words and punctuation
import os     # This is for performing file operations
import json   # This is for writing lists directly to files
import ast    # This is for reading lists back from files in a
              # safe manner; the eval() function is not secure
import shutil # This is for deleting non-empty folders

def tokenize_files(input_folder, output_folder):

    # This function "tokenizes" or splits up a raw text string into
    # an array of words and punctuation for later processing.
    
    # This function takes a folder path as input ("input_folder"),
    # and it processes each .txt file contained within the folder
    # path, creating a new tokenized .txt file of the same base
    # name within the folder path specified by "output_folder"

    # NOTE: "input_folder" should contain only .txt files and/or
    # subdirectories

    # Delete and re-create "token_folder"
    if os.path.exists(config.token_folder):
        shutil.rmtree(config.token_folder)
    os.makedirs(config.token_folder, exist_ok=True)

    for dir_path, _, file_names in os.walk(input_folder):
        for file_name in file_names:
            file_path = os.path.join(dir_path, file_name)

            # The code above recursively obtains the full file
            # path for each file within "folder" as well as
            # any subdirectory within the "folder" path
            
            # The code below is the process which each .txt file
            # is subjected to

            # Read the file and store the contents in "text"
            with open(file_path, "r") as f:
                text = f.read()

            # Split "text" into a sequence of tokens
            tokenized_text = nltk.word_tokenize(text)

            # Get the base name of the current .txt file
            base_name = os.path.basename(file_path)

            # Create the file path of the new tokenized file
            new_path = os.path.join(output_folder, base_name)

            # Create and write to the new file
            with open(new_path, "w") as f:
                json.dump(tokenized_text, f)

def generate_dictionary(input_folder, output_file_path):

    # This function generates a symbolic dictionary from the
    # the tokenized dataset, associating each token with its
    # own number

    # This function takes a folder path as input ("input_folder"),
    # which should be the same folder specified as "output_folder"
    # for the tokenize_files() function, and a file path
    # ("output_file_path") for the generated dictionary

    # Initialize dictionary as a Python dictionary
    dictionary = {"EMPTY_TOKEN": 0}
    next_index = 1

    # Non-recursively obtain the file path of each file in
    # "input_folder"
    # NOTE: This code does not check whether an item is a file
    # or a folder, as the folder generated by tokenize_files()
    # should not include any folders; this code assumes that
    # every item within "input_folder" is a .txt file
    for file_name in os.listdir(input_folder):
        file_path = os.path.join(input_folder, file_name)

        # Read the file and store the contents in "text"
        with open(file_path, "r") as f:
            text = f.read()
            
        # Convert the string "text" to a real Python list
        tokenized_text = ast.literal_eval(text)

        for token in tokenized_text:
            if dictionary.get(token) == None:
                dictionary[str(token)] = next_index
                next_index += 1
    
    # Create and write to dictionary file; overwrite if it
    # already exists
    with open(output_file_path, "w") as f:
        json.dump(dictionary, f)

def dictionarize_files(input_folder, output_folder):

    # This function uses the previously generated dictionary in
    # conjunction with the tokenized data to generate
    # dictionarized data, in which each token is now represented
    # by a unique number; for example, each "hello" token could
    # be replaced with the value 26, or each "goodbye" token could
    # be replaced with the value 89

    # This function takes a folder path as input ("input_folder"),
    # which should be the same folder specified as "output_folder"
    # for the tokenize_files() function, and a folder path
    # "output_folder" for the dictionarized data

    # Delete and re-create "dict_sequence_folder"
    if os.path.exists(config.dictionarized_folder):
        shutil.rmtree(config.dictionarized_folder)
    os.makedirs(config.dictionarized_folder, exist_ok=True)

    # Copy already-existing dictionary as "text"
    with open(config.dictionary_file, "r") as f:
        text = f.read()

    # Initialize dictionary as a Python dictionary
        dictionary = ast.literal_eval(text)

    for dir_path, _, file_names in os.walk(input_folder):
        for file_name in file_names:
            file_path = os.path.join(dir_path, file_name)

            # The code above recursively obtains the full file
            # path for each file within "folder" as well as
            # any subdirectory within the "folder" path

            # The code below is the process which each .txt file
            # is subjected to

            # Initialize dictionarized text as a list
            dictionarized_text = []

            # Read the file and store the contents in "text"
            with open(file_path, "r") as f:
                text = f.read()

            # Convert the string "text" to a real Python list
            tokenized_text = ast.literal_eval(text)

            for token in tokenized_text:
                dictionarized_text.append(dictionary[str(token)])

            # Get the base name of the current .txt file
            base_name = os.path.basename(file_path)

            # Create the file path of the new
            # dictionarized file
            new_path = os.path.join(output_folder, base_name)

            # Create and write to the new file
            with open(new_path, "w") as f:
                json.dump(dictionarized_text, f)